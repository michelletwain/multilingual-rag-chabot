# -*- coding: utf-8 -*-
"""
Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o3mpRtCeJylec1Q_v9MnYJUIR1J5jShW
"""

!huggingface-cli login

!pip install bitsandbytes

!pip install bitsandbytes>=0.39.0

!pip install accelerate

!pip install transformers sentence-transformers chromadb


from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModelForCausalLM, BitsAndBytesConfig
import torch
from sentence_transformers import SentenceTransformer
from chromadb.utils import embedding_functions
from chromadb.config import Settings
import chromadb

def load_llama():
    model_id = "meta-llama/Llama-3.1-8B-Instruct"
    quantization_config = BitsAndBytesConfig(load_in_4bit=True)
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map="auto",
        quantization_config=quantization_config,
        torch_dtype=torch.float16
    )
    return tokenizer, model

def generate_response(query, context, tokenizer, model):
    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=150)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def setup_vector_db():
    client = chromadb.PersistentClient(path="./chroma_data")
    ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="sentence-transformers/all-mpnet-base-v2")
    collection = client.create_collection(name="multilingual_docs", embedding_function=ef)
    return client, collection

def add_documents_to_db(collection):
    docs = [
        {"id": "1", "text": "Bonjour, je suis un chatbot multilingue.", "metadata": {"lang": "fr"}},
        {"id": "2", "text": "Hello, I am a multilingual chatbot.", "metadata": {"lang": "en"}}
    ]
    for doc in docs:
        collection.add(
            documents=[doc["text"]],
            metadatas=[doc["metadata"]],
            ids=[doc["id"]]
        )

def retrieve_context(collection, query, num_results=1):
    results = collection.query(
        query_texts=[query],
        n_results=num_results
    )
    if results['documents']:
        return results['documents'][0][0]
    return ""

def multilingual_chatbot():
    tokenizer, model = load_llama()
    client, collection = setup_vector_db()
    add_documents_to_db(collection)

    print("You can chat now! Type 'exit' to end the conversation.")
    while True:
        query = input("You: ")
        if query.lower() == "exit":
            print("Chat ended.")
            break
        context = retrieve_context(collection, query)
        response = generate_response(query, context, tokenizer, model)
        print(f"Chatbot: {response}")

if __name__ == "__main__":
    multilingual_chatbot()
